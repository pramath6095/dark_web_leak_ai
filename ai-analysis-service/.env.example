# ── Model mode ─────────────────────────────────────────────────────────────
USE_LOCAL_MODELS=true

# ── Local model paths (mounted via Docker volume at /models) ───────────────
LOCAL_CLASSIFIER_PATH=/models/bge-m3-zeroshot-v2.0
LOCAL_EMBEDDING_PATH=/models/bge-m3

# ── API fallback (used when USE_LOCAL_MODELS=false) ────────────────────────
OPENROUTER_API_KEY=your-openrouter-key-here
OPENROUTER_MODEL=cognitivecomputations/dolphin-mistral-24b-venice-edition:free
HF_API_KEY=your-huggingface-key-here
HF_CLASSIFIER_URL=https://router.huggingface.co/hf-inference/models/MoritzLaurer/bge-m3-zeroshot-v2.0
HF_EMBEDDING_URL=https://router.huggingface.co/hf-inference/models/BAAI/bge-m3

# ── Thresholds ─────────────────────────────────────────────────────────────
SIMILARITY_THRESHOLD=0.75
CLASSIFICATION_CONFIDENCE_THRESHOLD=0.65

# ── Classification labels (comma-separated, configurable) ─────────────────
CLASSIFICATION_LABELS=credential_leak,database_dump,internal_document,general_mention,irrelevant

# ── Chunking ───────────────────────────────────────────────────────────────
CHUNK_SIZE=400
CHUNK_OVERLAP=50

# ── Query service (for fetching search strings) ───────────────────────────
QUERY_SERVICE_URL=http://query-generator:8001

# ── Logging ────────────────────────────────────────────────────────────────
LOG_PATH=/logs/analysis.log
